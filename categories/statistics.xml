<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Charles Franzen (Posts about statistics)</title><link>http://charlesfranzen.com/</link><description></description><atom:link rel="self" type="application/rss+xml" href="http://charlesfranzen.com/categories/statistics.xml"></atom:link><language>en</language><lastBuildDate>Thu, 16 Mar 2017 00:19:31 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Multiple Regression in Python- Gradient Descent</title><link>http://charlesfranzen.com/posts/multiple-regression-in-python-gradient-descent/</link><dc:creator>Charles Franzen</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Multiple-Regression--Gradient-Descent"&gt;Multiple Regression- Gradient Descent&lt;a class="anchor-link" href="http://charlesfranzen.com/posts/multiple-regression-in-python-gradient-descent/#Multiple-Regression--Gradient-Descent"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;As the number of features used in regression increases, the matrix operations required by the closed-form solution become computationaly expensive, if not impossible.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://charlesfranzen.com/posts/multiple-regression-in-python-gradient-descent/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>machine-learning</category><category>python</category><category>regression</category><category>statistics</category><guid>http://charlesfranzen.com/posts/multiple-regression-in-python-gradient-descent/</guid><pubDate>Mon, 18 Jan 2016 09:46:22 GMT</pubDate></item><item><title>Markov Chains in Python</title><link>http://charlesfranzen.com/posts/markov-chains-in-python/</link><dc:creator>Charles Franzen</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Representing-Markov-Chains-in-Python-3"&gt;Representing Markov Chains in Python 3&lt;a class="anchor-link" href="http://charlesfranzen.com/posts/markov-chains-in-python/#Representing-Markov-Chains-in-Python-3"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Markov chains are random processes wherein state-changes occur according to some probablility function.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://charlesfranzen.com/posts/markov-chains-in-python/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>python</category><category>statistics</category><guid>http://charlesfranzen.com/posts/markov-chains-in-python/</guid><pubDate>Mon, 11 Jan 2016 13:17:50 GMT</pubDate></item><item><title>Simple Linear Regression in Python</title><link>http://charlesfranzen.com/posts/simple-linear-regression-in-python/</link><dc:creator>Charles Franzen</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Simple-Linear-Regression--Closed-Form"&gt;Simple Linear Regression- Closed Form&lt;a class="anchor-link" href="http://charlesfranzen.com/posts/simple-linear-regression-in-python/#Simple-Linear-Regression--Closed-Form"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;A Simple Linear Regression fits a line to data points with two dimensions. It does this by defining and then minimizing a cost function. One of the most common methods used is ordinary least squares (OLS), which minimizes the square of the residuals of a line plotted against the data points.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://charlesfranzen.com/posts/simple-linear-regression-in-python/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>python</category><category>regression</category><category>statistics</category><guid>http://charlesfranzen.com/posts/simple-linear-regression-in-python/</guid><pubDate>Sat, 02 Jan 2016 03:28:52 GMT</pubDate></item></channel></rss>